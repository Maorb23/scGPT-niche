{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Pre-trained Nicheformer Model for Downstream Tasks\n",
    "\n",
    "This notebook fine-tunes a pre-trained Nicheformer model for downstream tasks and stores predictions in an AnnData object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import anndata as ad\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "from nicheformer.models.nicheformer import Nicheformer\n",
    "from nicheformer.models._nicheformer_fine_tune import NicheformerFineTune\n",
    "from nicheformer.data.dataset import NicheformerDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the configuration parameters for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_path': 'path/to/your/data.h5ad',  # Path to your AnnData file\n",
    "    'technology_mean_path': 'path/to/technology_mean.npy',  # Path to technology mean file\n",
    "    'checkpoint_path': 'path/to/model/checkpoint.ckpt',  # Path to pre-trained model\n",
    "    'output_path': 'path/to/output/predictions.h5ad',  # Where to save results\n",
    "    'output_dir': 'path/to/output/directory',  # Directory for checkpoints\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 32,\n",
    "    'max_seq_len': 1500,\n",
    "    'aux_tokens': 30,\n",
    "    'chunk_size': 1000,\n",
    "    'num_workers': 4,\n",
    "    'precision': 32,\n",
    "    'max_epochs': 100,\n",
    "    'lr': 1e-4,\n",
    "    'warmup': 10,\n",
    "    'gradient_clip_val': 1.0,\n",
    "    'accumulate_grad_batches': 10,\n",
    "    \n",
    "    # Model parameters\n",
    "    'supervised_task': 'niche_regression',  # or whichever task\n",
    "    'extract_layers': [11],  # Which layers to extract features from\n",
    "    'function_layers': mean,  # Architecture of prediction head\n",
    "    'dim_prediction': 33, # dim of the output vector\n",
    "    'n_classes': 1,  # only foor classification tasks\n",
    "    'freeze': True,  # Whether to freeze backbone\n",
    "    'reinit_layers': False,\n",
    "    'extractor': False,\n",
    "    'regress_distribution': True,\n",
    "    'pool': 'mean',\n",
    "    'predict_density': False,\n",
    "    'ignore_zeros': False,\n",
    "    'organ': 'brain',\n",
    "    'label': 'X_niche_1'  # The target variable to predict\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Load data\n",
    "adata = ad.read_h5ad(config['data_path'])\n",
    "technology_mean = np.load(config['technology_mean_path'])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NicheformerDataset(\n",
    "    adata=adata,\n",
    "    technology_mean=technology_mean,\n",
    "    split='train',\n",
    "    max_seq_len=1500,\n",
    "    aux_tokens=config.get('aux_tokens', 30),\n",
    "    chunk_size=config.get('chunk_size', 1000),\n",
    "    metadata_fields = {\n",
    "        'obs': ['author_cell_type'],\n",
    "        #'obsm': ['X_niche_1'],\n",
    ")\n",
    "\n",
    "val_dataset = NicheformerDataset(\n",
    "    adata=adata,\n",
    "    technology_mean=technology_mean,\n",
    "    split='val',\n",
    "    max_seq_len=1500,\n",
    "    aux_tokens=config.get('aux_tokens', 30),\n",
    "    chunk_size=config.get('chunk_size', 1000),\n",
    "    metadata_fields = {\n",
    "        'obs': ['author_cell_type'],\n",
    "        #'obsm': ['X_niche_1'],\n",
    ")\n",
    "\n",
    "test_dataset = NicheformerDataset(\n",
    "    adata=adata,\n",
    "    technology_mean=technology_mean,\n",
    "    split='test',\n",
    "    max_seq_len=1500,\n",
    "    aux_tokens=config.get('aux_tokens', 30),\n",
    "    chunk_size=config.get('chunk_size', 1000),\n",
    "    metadata_fields = {\n",
    "        'obs': ['author_cell_type'],\n",
    "        #'obsm': ['X_niche_1'],\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config.get('num_workers', 4),\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config.get('num_workers', 4),\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config.get('num_workers', 4),\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Set Up Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = Nicheformer.load_from_checkpoint(checkpoint_path=config['checkpoint_path'], strict=False)\n",
    "\n",
    "# Create fine-tuning model\n",
    "fine_tune_model = NicheformerFineTune(\n",
    "    backbone=model,\n",
    "    supervised_task=config['supervised_task'],\n",
    "    extract_layers=config['extract_layers'],\n",
    "    function_layers=config['function_layers'],\n",
    "    lr=config['lr'],\n",
    "    warmup=config['warmup'],\n",
    "    max_epochs=config['max_epochs'],\n",
    "    dim_prediction=config['dim_prediction'],\n",
    "    n_classes=config['n_classes'],\n",
    "    baseline=config['baseline'],\n",
    "    freeze=config['freeze'],\n",
    "    reinit_layers=config['reinit_layers'],\n",
    "    extractor=config['extractor'],\n",
    "    regress_distribution=config['regress_distribution'],\n",
    "    pool=config['pool'],\n",
    "    predict_density=config['predict_density'],\n",
    "    ignore_zeros=config['ignore_zeros'],\n",
    "    organ=config.get('organ', 'unknown'),\n",
    "    label=config['label'],\n",
    "    without_context=True\n",
    ")\n",
    "\n",
    "# Configure trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config['max_epochs'],\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    default_root_dir=config['output_dir'],\n",
    "    precision=config.get('precision', 32),\n",
    "    gradient_clip_val=config.get('gradient_clip_val', 1.0),\n",
    "    accumulate_grad_batches=config.get('accumulate_grad_batches', 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "trainer.fit(\n",
    "    model=fine_tune_model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "print(\"Testing the model...\")\n",
    "test_results = trainer.test(\n",
    "    model=fine_tune_model,\n",
    "    dataloaders=test_loader\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "print(\"Getting predictions...\")\n",
    "predictions = trainer.predict(fine_tune_model, dataloaders=test_loader)\n",
    "predictions = [torch.cat([p[0] for p in predictions]).cpu().numpy(),\n",
    "              torch.cat([p[1] for p in predictions]).cpu().numpy()]\n",
    "if 'regression' in config['supervised_task']:\n",
    "    predictions = predictions[0]  # For regression both values are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Store predictions in AnnData object\n",
    "prediction_key = f\"predictions_{config.get('label', 'X_niche_1')}\"\n",
    "test_mask = adata.obs.nicheformer_split == 'test'\n",
    "\n",
    "if 'classification' in config['supervised_task']:\n",
    "    # For classification tasks\n",
    "    adata.obs.loc[test_mask, f\"{prediction_key}_class\"] = predictions[0]\n",
    "    adata.obs.loc[test_mask, f\"{prediction_key}_class_probs\"] = predictions[1]\n",
    "else:\n",
    "    # For regression tasks\n",
    "    adata.obs.loc[test_mask, prediction_key] = predictions\n",
    "\n",
    "# Store test metrics\n",
    "for metric_name, value in test_results[0].items():\n",
    "    adata.uns[f\"{prediction_key}_metrics_{metric_name}\"] = value\n",
    "\n",
    "# Save updated AnnData\n",
    "adata.write_h5ad(config['output_path'])\n",
    "\n",
    "print(f\"Results saved to {config['output_path']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
